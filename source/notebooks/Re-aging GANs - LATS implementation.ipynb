{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40c49bb8",
   "metadata": {},
   "source": [
    "# [Re-Aging GAN: Toward Personalized Face Age Transformation](https://openaccess.thecvf.com/content/ICCV2021/papers/Makhmudkhujaev_Re-Aging_GAN_Toward_Personalized_Face_Age_Transformation_ICCV_2021_paper.pdf)\n",
    "\n",
    "This kernel is the working of the above mentioned paper.\n",
    "\n",
    "> The paper doesn't provide enough data to build the entire paper, so I will be extracting some information from the [Lifespan Age Transformation Synthesis](https://arxiv.org/pdf/2003.09764.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffbe6c90",
   "metadata": {},
   "source": [
    "## Proposed Method\n",
    "The authors propose the following solution.\n",
    "\n",
    "### Overview\n",
    "\n",
    "- Let $\\mathcal{X}$ and $\\mathcal{Y}$ be the sets of images and possible ages respectively.\n",
    "- Given a face image $x\\in\\mathcal{X}$ and target age randomly drawn from $y'\\in\\mathcal{Y}$.\n",
    "- We need to train a single geenrator $G$ such that it can generate a face image $x'$ of a particular age $y'$ corresponding to the identity in $x$.\n",
    "\n",
    "### Identity Encoder\n",
    "- Given an image $x$ for age transformation, the identity encoder $Enc$ extracts the identity-related features $f_{id}$ of the image, where $f_{id} = Enc(x)$.\n",
    "    - The encoder provides features, that supply facial feature at the local lebel and general inforamtion on the face shape.\n",
    "    - These features are necessary to generate the same looking face shape.\n",
    "- The paper requires the encoder to focus only on the facial details and as a result, they generate a mask for the images and process the masked images as a result.\n",
    "    - By this, we can get the background details, and they can be maintained unchanged, and work on the facial values.\n",
    "\n",
    "> At the architecture level, the identity encoder is designed to have an image-to-feature level convlutional layer followed by downsampling residual blocks.\n",
    "\n",
    "### Age Modulator\n",
    "- Age modulator AM is constructed in the form of a CNN.\n",
    "- Our age input space, $\\mathcal{Z}$, is represented by a $50×n$ element vector where n is the number of age classes. When the input age class is $i$, we generate a vector $z_i ∈ \\mathcal{Z}$ as\n",
    "$$z_i = \\mathbb{1}_i + v,~~~~v ∼ \\mathcal{N}(0, 0.2^2·I)$$\n",
    "- This takes the identity features $f_{id}$ from the encoder and, by considering given age informtion $y'$, outputs it's reshaped version $f_{aw} = AM(f_{id}, y')$, where $f_{aw}$ is an element age-aware vector.\n",
    "- To embed target age into $AM$, we add __conditional batch normalization (CBN)__ layers.\n",
    "    - This is used as a way to encode label information into the network\n",
    "- The $AM$ itself acts as a downsampling layer with CBN technique producing a compact feature vector used to modulate the decoder layers.\n",
    "\n",
    "### Decoder\n",
    "- Takes the identity features ($f_{id}$) and the age aware features ($f_{aw}$) and produces age-transformed face image by $x'=Dec(f_{id}, f_{aw})$.\n",
    "    - They make the age-aware features to self-guide the decoding process through the modulation operations on unshaped identity features.\n",
    "- The features are modulated by __adaptive instance normalization (AdaIN)__ layers.\n",
    "- Then they again mask the image so as to remove irrelavent information added to facial images and then finally add back the background.\n",
    "\n",
    "### Discriminator\n",
    "- The discriminator follows a multi-task classification. Hence, the last fully connected layer has a number of output branches to classifify multiple age classes.\n",
    "- By performing binary classification, each of the braches learns to determine the validity of the image being real $x$ or fake $x'$ of it's age domain.\n",
    "\n",
    "## Optimization\n",
    "\n",
    "- The target of the model is to produce images where the identity of the input is preserved, whereas a target age is accurately represented.\n",
    "- For this purpose, there are three losses which where introduced.\n",
    "    - Adversarial Loss\n",
    "    - Reconstruction Loss\n",
    "    - Cycle Consistency Loss\n",
    "- The framework operates on 3 input formation.\n",
    "    - an input image $x$,\n",
    "    - it's corresponding age label $y$, and\n",
    "    - randomly sampled target age $y'$ into which the input should be transformed.\n",
    "- $G$ will produce age-transformed $x'$, $x_{rec}$ is the reconstructed image, and $x_{cycle}$ is the cycle consistency images.\n",
    "\n",
    "$$x'=G(x,y'),~~~~x_{rec}=G(x,y),~~~~x_{cycle}=G(x',y)$$\n",
    "\n",
    "### Adversarial Loss\n",
    "\n",
    "The output of the discriminator corresponds to the particular age domain. Hence, we can think that adversarial loss is conditioned on the age class. We use an adversarial loss is formulated as:\n",
    "\n",
    "$$\\mathcal{L}_{adv}(G,D) = \\mathbb{E}_{x, y}[\\log D_y(x)] + \\mathbb{E}_{x, y'}[\\log(1 - D_{y'}(x'))]$$\n",
    "\n",
    "To understand this loss function, if we look at the LATS paper, we get an idea of what is needed.\n",
    "\n",
    "They formulate it as:\n",
    "$$\\mathcal{L}_{adv}(G,D) = \\mathbb{E}_{x, s}[\\log D_s(x)] + \\mathbb{E}_{x\n",
    ", t}[\\log(1 - D_{t}(x'))]$$\n",
    "where, $s$ is the source age-class and $t$ is the target age-class.\n",
    "\n",
    "As per the LATS paper, they define $D_i$ as the $i^{th}$ output of the discriminator.\n",
    "\n",
    "### Reconstruction Loss\n",
    "\n",
    "While training $G$, we have to consider the case whe the age $y$ of the input image and the target image $y'$ belong to the same age group ($y = y'$). In this case, the age-transformed image $x'$ should be as close as possible to $x$.\n",
    "\n",
    "$$\\mathcal{L}_{rec}(G) = \\| x - x_{rec}\\| _1$$.\n",
    "\n",
    "### Cycle-Consistency Loss\n",
    "\n",
    "We can train $G$ to generate images that are realistic and accurate in terms of target age by minimizing the adversarial and reconstruction losses.\n",
    "\n",
    "Since, we try to generate $x$ back from $x_{cycle}$, we would ideally want both of them to be as similar as possible.\n",
    "\n",
    "### Full Objective\n",
    "\n",
    "$$\\min_G\\max_D[\\lambda_{adv}\\mathcal{L}_{adv}(G, D) + \\lambda_{rec}\\mathcal{rec}(G) + \\lambda_{cycle}\\mathcal{L}_{cyc}]$$\n",
    "\n",
    "## Experimental Setup (Prescribed hyperparameters)\n",
    "\n",
    "- They trained their model with a __batch size of 8__ for __30 epochs__ on a single __NVIDIA Titan RTX GPU__.\n",
    "- As an optimizer, they use __Adam__ with a momentum parameters settings $\\beta_1=0.0$ and $\\beta_2 = 0.99$ and a __learning rate of $10^{-4}$__.\n",
    "- They also add R1 Regularization.\n",
    "- They also have a learning rate scheduler for both the generator and the discriminator. \n",
    "- In the beginning of $10$ epochs, we train the model with $\\lambda_{rec}=10$, $\\lambda{cyc} = 1$, and $\\lambda{adv} = 1$ for reconstruction, cycle-consistency, and adversarial losses, respectively. Thereafter, we reduce $\\lambda_{rec}=1$ because such training leads to better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c7773f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Author: Aneesh Aparajit G\n",
      "\n",
      "torch      : 1.12.1\n",
      "torchvision: 0.13.1\n",
      "numpy      : 1.21.5\n",
      "cv2        : 4.6.0\n",
      "matplotlib : 3.5.2\n",
      "scipy      : 1.7.3\n",
      "PIL        : 9.2.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%load_ext watermark\n",
    "%watermark -a \"Aneesh Aparajit G\" -p torch,torchvision,numpy,cv2,matplotlib,scipy,PIL"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch env",
   "language": "python",
   "name": "pytorch_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
