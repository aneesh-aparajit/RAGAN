{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "293f478d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, Dataset\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7c48ba0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['35', '65', '5']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir('../../data/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a35f73",
   "metadata": {},
   "source": [
    "- 5 -> 0\n",
    "- 35 -> 1\n",
    "- 65 -> 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "88539e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_images = None\n",
    "input_ages = torch.tensor([])\n",
    "output_images = None\n",
    "output_ages = torch.tensor([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "60e2a5fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "age: 35 -> len: 27\n",
      "age: 65 -> len: 30\n",
      "age: 5 -> len: 28\n"
     ]
    }
   ],
   "source": [
    "for age_group in os.listdir('../../data/'):\n",
    "    print(f'age: {age_group} -> len: {len(os.listdir(f\"../../data/{age_group}\"))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ce1e6f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_encode(age):\n",
    "    if age == '5':\n",
    "        return 0\n",
    "    elif age == '35':\n",
    "        return 1\n",
    "    return 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1b899e1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Age: 35: 100%|████████████████████████████████████| 3/3 [01:44<00:00, 34.96s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_images: torch.Size([1566, 3, 256, 256]), input_age: torch.Size([1566]), output_images: torch.Size([1566, 3, 256, 16]), output ages: torch.Size([1566])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Age: 65: 100%|███████████████████████████████████| 3/3 [05:43<00:00, 114.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_images: torch.Size([3216, 3, 256, 256]), input_age: torch.Size([3216]), output_images: torch.Size([3216, 3, 256, 16]), output ages: torch.Size([3216])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Age: 5: 100%|████████████████████████████████████| 3/3 [09:14<00:00, 184.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_images: torch.Size([4812, 3, 256, 256]), input_age: torch.Size([4812]), output_images: torch.Size([4812, 3, 256, 16]), output ages: torch.Size([4812])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for age1 in os.listdir('../../data/'):\n",
    "    for age2 in tqdm(os.listdir('../../data/'), desc=f'Age: {age1}'):\n",
    "        if age1 != age2:\n",
    "            for file1 in os.listdir(f'../../data/{age1}'):\n",
    "                img1 = cv2.imread(f'../../data/{age1}/{file1}', cv2.IMREAD_COLOR)\n",
    "                img1 = cv2.cvtColor(img1, cv2.COLOR_BGR2RGB)\n",
    "                img1.resize((256, 256, 3))\n",
    "                img1 = torch.tensor(img1).view(1, 3, 256, -1)\n",
    "                for file2 in os.listdir(f'../../data/{age2}'):\n",
    "                    img2 = cv2.imread(f'../../data/{age2}/{file2}', cv2.IMREAD_COLOR)\n",
    "                    img2 = cv2.cvtColor(img2, cv2.COLOR_BGR2RGB)\n",
    "                    img2.resize((64, 64, 3))\n",
    "                    img2 = torch.Tensor(img2).view(1, 3, 256, -1)\n",
    "                    \n",
    "                    if input_images is None:\n",
    "                        input_images = img1\n",
    "                    else:\n",
    "                        input_images = torch.cat([input_images, img1], axis=0)\n",
    "                    input_ages = torch.cat([input_ages, torch.Tensor([get_encode(age1)])])\n",
    "                    \n",
    "                    if output_images is None:\n",
    "                        output_images = img2\n",
    "                    else:\n",
    "                        output_images = torch.cat([output_images, img2], axis=0)\n",
    "                    output_ages = torch.cat([output_ages, torch.Tensor([get_encode(age2)])])\n",
    "    print(f'input_images: {input_images.shape}, input_age: {input_ages.shape}, output_images: {output_images.shape}, output ages: {output_ages.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "22b02561",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(input_images, '../../data/input_images.pth')\n",
    "torch.save(input_ages, '../../data/input_ages.pth')\n",
    "torch.save(input_images, '../../data/output_images.pth')\n",
    "torch.save(input_ages, '../../data/output_ages.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b089e6a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch (Python 3.9)",
   "language": "python",
   "name": "pytorch_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
