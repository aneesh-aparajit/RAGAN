{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "293f478d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, Dataset\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c48ba0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['35', '65', '5']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir('../../data/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a35f73",
   "metadata": {},
   "source": [
    "- 5 -> 0\n",
    "- 35 -> 1\n",
    "- 65 -> 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ff99a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.remove('../../data/.DS_Store')\n",
    "# os.remove('../../data/35/.DS_Store')\n",
    "# os.remove('../../data/5/.DS_Store')\n",
    "# os.remove('../../data/65/.DS_Store')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "88539e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_images = None\n",
    "input_ages = torch.tensor([])\n",
    "output_images = None\n",
    "output_ages = torch.tensor([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "60e2a5fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "age: 35 -> len: 10\n",
      "age: 65 -> len: 10\n",
      "age: 5 -> len: 11\n"
     ]
    }
   ],
   "source": [
    "for age_group in os.listdir('../../data/'):\n",
    "    print(f'age: {age_group} -> len: {len(os.listdir(f\"../../data/{age_group}\"))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c62cb28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "age: 35, img: 42_0_0_20170105172350421.jpg, size: (200, 200, 3)\n",
      "age: 35, img: 42_0_0_20170105173005714.jpg, size: (200, 200, 3)\n",
      "age: 35, img: 42_0_0_20170105172318206.jpg, size: (200, 200, 3)\n",
      "age: 35, img: 42_0_0_20170117120422779.jpg, size: (200, 200, 3)\n",
      "age: 35, img: 42_0_0_20170111200657340.jpg, size: (200, 200, 3)\n",
      "age: 35, img: 42_0_0_20170104204536019.jpg, size: (200, 200, 3)\n",
      "age: 35, img: 42_0_0_20170104205801060.jpg, size: (200, 200, 3)\n",
      "age: 35, img: 42_0_0_20170109004710455.jpg, size: (200, 200, 3)\n",
      "age: 35, img: 42_0_0_20170104170602137 (1).jpg, size: (200, 200, 3)\n",
      "age: 35, img: 42_0_0_20170104170602137.jpg, size: (200, 200, 3)\n",
      "age: 65, img: 60_0_0_20170104184725621.jpg, size: (200, 200, 3)\n",
      "age: 65, img: 60_0_0_20170104184751214.jpg, size: (200, 200, 3)\n",
      "age: 65, img: 60_0_0_20170111171747553.jpg, size: (200, 200, 3)\n",
      "age: 65, img: 60_0_0_20170111201620287.jpg, size: (200, 200, 3)\n",
      "age: 65, img: 60_0_0_20170103182814970.jpg, size: (200, 200, 3)\n",
      "age: 65, img: 60_0_0_20170104170240505.jpg, size: (200, 200, 3)\n",
      "age: 65, img: 60_0_0_20170103182814970 (1).jpg, size: (200, 200, 3)\n",
      "age: 65, img: 60_0_0_20170111171747549.jpg, size: (200, 200, 3)\n",
      "age: 65, img: 60_0_0_20170111171747538.jpg, size: (200, 200, 3)\n",
      "age: 65, img: 60_0_0_20170111201520536.jpg, size: (200, 200, 3)\n",
      "age: 5, img: 10_0_0_20170110220235233.jpg, size: (200, 200, 3)\n",
      "age: 5, img: 10_0_0_20170103233459275.jpg, size: (200, 200, 3)\n",
      "age: 5, img: 10_0_0_20170110221714752.jpg, size: (200, 200, 3)\n",
      "age: 5, img: 10_0_0_20170110215927291.jpg, size: (200, 200, 3)\n",
      "age: 5, img: 10_0_0_20161220222308131.jpg, size: (200, 200, 3)\n",
      "age: 5, img: 10_0_0_20170110220033115.jpg, size: (200, 200, 3)\n",
      "age: 5, img: 10_0_0_20170103200329407.jpg, size: (200, 200, 3)\n",
      "age: 5, img: 10_0_0_20170110220111082.jpg, size: (200, 200, 3)\n",
      "age: 5, img: 10_0_0_20170104013211746.jpg, size: (200, 200, 3)\n",
      "age: 5, img: 10_0_0_20170103200522151.jpg, size: (200, 200, 3)\n",
      "age: 5, img: 10_0_0_20170110220530650.jpg, size: (200, 200, 3)\n"
     ]
    }
   ],
   "source": [
    "for age in os.listdir('../../data/'):\n",
    "    for img in os.listdir('../../data/'+age+'/'):\n",
    "        _img = cv2.imread('../../data/'+age+'/'+img)\n",
    "        print(f'age: {age}, img: {img}, size: {_img.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ce1e6f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_encode(age):\n",
    "    if age == '5':\n",
    "        return 0\n",
    "    elif age == '35':\n",
    "        return 1\n",
    "    return 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1b899e1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Age: 35: 100%|█████████████████████████████████████████████████████████████████████| 3/3 [00:06<00:00,  2.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_images: torch.Size([210, 3, 256, 256]), input_age: torch.Size([210]), output_images: torch.Size([210, 3, 256, 256]), output ages: torch.Size([210])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Age: 65: 100%|█████████████████████████████████████████████████████████████████████| 3/3 [00:18<00:00,  6.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_images: torch.Size([420, 3, 256, 256]), input_age: torch.Size([420]), output_images: torch.Size([420, 3, 256, 256]), output ages: torch.Size([420])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Age: 5: 100%|██████████████████████████████████████████████████████████████████████| 3/3 [00:32<00:00, 10.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_images: torch.Size([640, 3, 256, 256]), input_age: torch.Size([640]), output_images: torch.Size([640, 3, 256, 256]), output ages: torch.Size([640])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for age1 in os.listdir('../../data/'):\n",
    "    for age2 in tqdm(os.listdir('../../data/'), desc=f'Age: {age1}'):\n",
    "        if age1 != age2:\n",
    "            for file1 in os.listdir(f'../../data/{age1}'):\n",
    "                img1 = cv2.imread(f'../../data/{age1}/{file1}', cv2.IMREAD_COLOR)\n",
    "                img1 = cv2.cvtColor(img1, cv2.COLOR_BGR2RGB)\n",
    "                img1 = cv2.resize(img1, None, fx=1.28, fy=1.28)\n",
    "                img1 = torch.tensor(img1).permute(2, 1, 0).unsqueeze(0)\n",
    "                for file2 in os.listdir(f'../../data/{age2}'):\n",
    "                    img2 = cv2.imread(f'../../data/{age2}/{file2}', cv2.IMREAD_COLOR)\n",
    "                    img2 = cv2.cvtColor(img2, cv2.COLOR_BGR2RGB)\n",
    "                    img2 = cv2.resize(img2, None, fx=1.28, fy=1.28)\n",
    "                    img2 = torch.Tensor(img2).permute(2, 1, 0).unsqueeze(0)\n",
    "                    \n",
    "                    if input_images is None:\n",
    "                        input_images = img1\n",
    "                    else:\n",
    "                        input_images = torch.cat([input_images, img1], axis=0)\n",
    "                    input_ages = torch.cat([input_ages, torch.Tensor([get_encode(age1)])])\n",
    "                    \n",
    "                    if output_images is None:\n",
    "                        output_images = img2\n",
    "                    else:\n",
    "                        output_images = torch.cat([output_images, img2], axis=0)\n",
    "                    output_ages = torch.cat([output_ages, torch.Tensor([get_encode(age2)])])\n",
    "    print(f'input_images: {input_images.shape}, input_age: {input_ages.shape}, output_images: {output_images.shape}, output ages: {output_ages.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "22b02561",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(input_images, '../../data/input_images.pth')\n",
    "torch.save(input_ages, '../../data/input_ages.pth')\n",
    "torch.save(input_images, '../../data/output_images.pth')\n",
    "torch.save(input_ages, '../../data/output_ages.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f428e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (PyTorch)",
   "language": "python",
   "name": "pytorch_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
